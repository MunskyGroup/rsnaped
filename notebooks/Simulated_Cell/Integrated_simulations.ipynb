{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os; from os import listdir; from os.path import isfile, join\n",
    "import re  \n",
    "#from skimage import io \n",
    "from skimage.io import imread\n",
    "# To manipulate arrays\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "from timeit import default_timer as timer\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import pathlib\n",
    "import sys\n",
    "from skimage.exposure import rescale_intensity\n",
    "import rsnapsim as rss\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Deffining directories\n",
    "current_dir = pathlib.Path().absolute()\n",
    "sequences_dir = current_dir.parents[1].joinpath('DataBases','gene_files')\n",
    "video_dir = current_dir.parents[1].joinpath('DataBases','videos_for_sim_cell')\n",
    "#trajectories_dir = current_dir.parents[1].joinpath('DataBases','rsnapsim_simulations','bactin_ssa.npy')\n",
    "trajectories_dir = current_dir.parents[1].joinpath('DataBases','rsnapsim_simulations','kdm5b_ssa.npy')\n",
    "rsnaped_dir = current_dir.parents[1].joinpath('rsnaped')\n",
    "gene_file = current_dir.parents[1].joinpath('DataBases','gene_files','KDM5B_withTags.txt')\n",
    "masks_dir = current_dir.parents[1].joinpath('DataBases','masks_for_sim_cell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importing rSNAPed\n",
    "sys.path.append(str(rsnaped_dir))\n",
    "import rsnaped as rsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters that need to be tested. \n",
    "number_of_simulated_cells = 8*2 # PLEASE TEST MIN 1 MAX 10\n",
    "number_spots_per_cell = 40     # PLEASE TEST MIN 5 MAX 200\n",
    "simulation_time_in_sec = 50     # PLEASE TEST MIN 10 MAX 100\n",
    "diffusion_coefficient = 0.7    # PLEASE TEST MIN 0.1 MAX 2\n",
    "min_percentage_time_tracking = 0.4            # (normalized) minimum time to consider a trajectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "intensity_calculation_method = 'disk_donut'  # options are : 'total_intensity' and 'disk_donut' 'gaussian_fit'\n",
    "mask_selection_method = 'max_area' # options are : 'max_spots' and 'max_area' \n",
    "use_optimization_for_tracking = 1 # 0 not using, 1 is using optimization\n",
    "frame_selection_empty_video = 'shuffle' # Options are: 'constant' , 'shuffle' and 'loop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "particle_size = 5 # spot size for the simulation and tracking.\n",
    "elongation_rate = 5\n",
    "initiation_rate = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_scale_ch0 = 300\n",
    "intensity_scale_ch1 = 200\n",
    "intensity_scale_ch2 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_detection_size = particle_size+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsnapsim_ssa(gene_file,ke,ki,frames=300,frame_rate=1,n_traj=20):\n",
    "    '''\n",
    "    This function uses rsnapsim to simulate the single-molecule translation dynamcis of any gene.\n",
    "    Inputs:\n",
    "    gene_file : str, with the location of a fasta file.\n",
    "    ke : float, elongation rate.\n",
    "    ki: float, initiation rate.\n",
    "    frames: int, total number of simulation frames in seconds.\n",
    "    n_traj: int, number of trajectories to simulate\n",
    "    frame_rate : int, frame rate per second\n",
    "    Outputs:\n",
    "    ssa_int : NumPy array with dimenssions [Time_points, simulated_trajectories]\n",
    "    '''\n",
    "    poi_strs, poi_objs, tagged_pois,raw_seq = rss.seqmanip.open_seq_file(str(gene_file))\n",
    "    gene_obj = tagged_pois['1'][0]\n",
    "    gene_obj.ke_mu = ke\n",
    "    rss.solver.protein = gene_obj #pass the protein object\n",
    "    t_burnin = 1000\n",
    "    t = np.linspace(0,t_burnin+frames,(t_burnin+frames+1)*(frame_rate))\n",
    "    number_probes = np.amax(gene_obj.probe_vec)\n",
    "    ints = []\n",
    "    #for i in tq.tqdm(range(n_traj)):\n",
    "    counter = 0\n",
    "    while counter < n_traj:\n",
    "      ssa_solution = rss.solver.solve_ssa(gene_obj.kelong, t, ki=ki, kt = ke, low_memory=True,record_stats=False,n_traj=1)\n",
    "      ssa_int =  ssa_solution.intensity_vec[0,t_burnin*frame_rate:-1,:]\n",
    "      if np.mean(ssa_int)> 1:\n",
    "        ints.append(ssa_int)\n",
    "        counter +=1\n",
    "    ssa = np.array(ints).reshape(n_traj,frames) #flatten the lists back to a numpy array\n",
    "\n",
    "    ssa_ump = ssa/number_probes\n",
    "    return ssa, ssa_ump, gene_obj, t,number_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_simulated_cells(current_dir, video_dir,masks_dir=None,ke=3,ki=0.03,gene_file =None, trajectories_dir=None, number_of_simulated_cells=3,number_spots_per_cell=80,simulation_time_in_sec =100,step_size_in_sec=1,particle_size=5, diffusion_coefficient =1,path_to_rSNAPsim= None, path_to_save_output='temp',intensity_calculation_method='gaussian_fit',frame_selection_empty_video=frame_selection_empty_video):\n",
    "    spot_size = particle_size\n",
    "    spot_sigma = 1\n",
    "    # Code that creates the folder to store results.\n",
    "    diffusion_coefficient_string = str(diffusion_coefficient).replace('.','_')\n",
    "    directory_name = 'Simulation_V2__'+'ns_'+str(number_spots_per_cell) +'_diff_'+ diffusion_coefficient_string \n",
    "    path_to_save_output = 'temp'\n",
    "    save_to_path =  current_dir.joinpath(path_to_save_output , directory_name )\n",
    "    if not os.path.exists(str(save_to_path)):\n",
    "        os.makedirs(str(save_to_path))\n",
    "    else:\n",
    "        shutil.rmtree(str(save_to_path))\n",
    "        os.makedirs(str(save_to_path))\n",
    "    counter = 0\n",
    "    ## Main loop that creates each cell and dataframe\n",
    "    for cell_number in range (0, number_of_simulated_cells):\n",
    "        output_directory_name = str(video_dir)\n",
    "        list_files_names = sorted([f for f in listdir(output_directory_name) if isfile(join(output_directory_name, f)) and ('.tif') in f], key=str.lower)  # reading all tif files in the folder\n",
    "        list_files_names.sort(key=lambda f: int(re.sub('\\D', '', f)))  # sorting the index in numerical order\n",
    "        path_files = [ str(video_dir.joinpath(f).resolve()) for f in list_files_names ] # creating the complete path for each file\n",
    "        video_path = path_files[counter]        \n",
    "        video = imread(str(video_path)) \n",
    "        # Reducing in a half the intensity in the original video\n",
    "        video = video//2\n",
    "        # Loading the mask image\n",
    "        if not (masks_dir is None):\n",
    "            mask_image=imread(masks_dir.joinpath('mask_cell_shape_'+str(counter)+'.tif'))        \n",
    "        counter +=1\n",
    "        if counter>=len(path_files):\n",
    "            counter =0\n",
    "        if not (trajectories_dir is None ):\n",
    "            # Loading trajectories from file\n",
    "            ssa_trajectories = np.load(str(trajectories_dir))\n",
    "            random_index_ch0 = np.random.randint(low=0, high=ssa_trajectories.shape[0]-1, size=(number_spots_per_cell,))\n",
    "            random_index_ch1 = np.random.randint(low=0, high=ssa_trajectories.shape[0]-1, size=(number_spots_per_cell,))\n",
    "            random_index_ch2 = np.random.randint(low=0, high=ssa_trajectories.shape[0]-1, size=(number_spots_per_cell,))\n",
    "            simulated_trajectories_ch0 = ssa_trajectories[random_index_ch0,0:simulation_time_in_sec:step_size_in_sec]\n",
    "            simulated_trajectories_ch1 = ssa_trajectories[random_index_ch1,0:simulation_time_in_sec:step_size_in_sec]\n",
    "            simulated_trajectories_ch2 =  ssa_trajectories[random_index_ch2,0:simulation_time_in_sec:step_size_in_sec]\n",
    "        else:\n",
    "            # Simulations for intensity\n",
    "            ssa1,ssa1_ump,_,_,number_probes = rsnapsim_ssa(gene_file,ke,ki,frames=simulation_time_in_sec,frame_rate=1,n_traj=number_spots_per_cell) # rss.ssa_solver(n_traj = number_spots_per_cell, start_time=starting_time,tf=starting_time+n_frames, tstep=starting_time+n_frames,k_elong_mean=3, k_initiation=.03)  # tstep = total number of steps including the burnin time \n",
    "            simulated_trajectories_ch1 = ssa1_ump\n",
    "            ssa2,ssa2_ump,_,_,number_probes =  rsnapsim_ssa(gene_file,ke,ki,frames=simulation_time_in_sec,frame_rate=1,n_traj=number_spots_per_cell) # rss.ssa_solver(n_traj = number_spots_per_cell, start_time=starting_time,tf=starting_time+n_frames, tstep=starting_time+n_frames,k_elong_mean=3, k_initiation=.03)  # tstep = total number of steps including the burnin time \n",
    "            simulated_trajectories_ch2 = ssa2_ump\n",
    "            simulated_trajectories_ch0 = simulated_trajectories_ch1\n",
    "        #simulated_trajectories_ch0 = None\n",
    "        # Running the cell simulation\n",
    "        saved_file_name = str(save_to_path.joinpath('sim_cell_'+str(cell_number)))\n",
    "        tensor_video , spot_positions_movement, DataFrame_particles_intensities = rsp.SimulatedCell( base_video=video, mask_image=mask_image, number_spots = number_spots_per_cell, number_frames=simulation_time_in_sec, step_size=step_size_in_sec, diffusion_coefficient =diffusion_coefficient, simulated_trajectories_ch0=simulated_trajectories_ch0, size_spot_ch0=spot_size, spot_sigma_ch0=spot_sigma, simulated_trajectories_ch1=simulated_trajectories_ch1, size_spot_ch1=spot_size, spot_sigma_ch1=spot_sigma, simulated_trajectories_ch2=simulated_trajectories_ch2, size_spot_ch2=spot_size, spot_sigma_ch2=spot_sigma, ignore_ch0=0,ignore_ch1=0, ignore_ch2=1,save_as_tif_uint8=0,save_as_tif =1,save_as_gif=0, save_dataframe=1, saved_file_name=saved_file_name,create_temp_folder = False, intensity_calculation_method=intensity_calculation_method,perform_video_augmentation=1,frame_selection_empty_video=frame_selection_empty_video ,intensity_scale_ch0 = intensity_scale_ch0,intensity_scale_ch1 = intensity_scale_ch1,intensity_scale_ch2 = intensity_scale_ch2).make_simulation()      \n",
    "        #print ('The results are saved in folder: ', saved_file_name)\n",
    "    return save_to_path, simulated_trajectories_ch0, simulated_trajectories_ch1, simulated_trajectories_ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the simulation\n",
    "start = timer()\n",
    "output_directory_name, simulated_trajectories_ch0, simulated_trajectories_ch1, simulated_trajectories_ch2 = fun_simulated_cells(current_dir,video_dir,masks_dir,ke=elongation_rate, ki=initiation_rate,trajectories_dir=None,gene_file= gene_file, number_of_simulated_cells=number_of_simulated_cells,number_spots_per_cell=number_spots_per_cell,simulation_time_in_sec =simulation_time_in_sec,step_size_in_sec=1,particle_size=particle_size, diffusion_coefficient=diffusion_coefficient,path_to_rSNAPsim= None,intensity_calculation_method=intensity_calculation_method,frame_selection_empty_video=frame_selection_empty_video)\n",
    "end = timer()\n",
    "print('Time to generate simulated data:',round(end - start), ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = str(output_directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reads the folder with the results and import the simulations as lists\n",
    "list_files_names = sorted([f for f in listdir(output_directory_name) if isfile(join(output_directory_name, f)) and ('.tif') in f], key=str.lower)  # reading all tif files in the folder\n",
    "list_files_names.sort(key=lambda f: int(re.sub('\\D', '', f)))  # sorting the index in numerical order\n",
    "path_files = [ str(output_directory_name.joinpath(f).resolve()) for f in list_files_names ] # creating the complete path for each file\n",
    "\n",
    "# Reading the microscopy data\n",
    "list_videos_original = [imread(f)[:,:,:,:] for f in  path_files] # List with all the videos\n",
    "nimg = number_of_simulated_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling images\n",
    "rescale_video = False\n",
    "if rescale_video == True:\n",
    "    list_videos = []\n",
    "    number_channels = list_videos_original[0].shape[-1]\n",
    "    number_z_slices = list_videos_original[0].shape[0]\n",
    "    for i in range (0,nimg ):\n",
    "        temp_img = np.zeros_like(list_videos_original[0])\n",
    "        for j in range(0,number_channels):\n",
    "            temp_img[:,:,:,j] =  np.asarray( [ rescale_intensity(list_videos_original[i][z,:,:,j], in_range='image', out_range='dtype')  for z in range (0, number_z_slices)]  )\n",
    "        list_videos.append(temp_img)\n",
    "else:\n",
    "    list_videos = list_videos_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reads the dataframes\n",
    "list_files_dfnames = sorted([f for f in listdir(output_directory_name) if isfile(join(output_directory_name, f)) and ('.csv') in f], key=str.lower)  # reading all tif files in the folder\n",
    "list_files_dfnames.sort(key=lambda f: int(re.sub('\\D', '', f)))  # sorting the index in numerical order\n",
    "path_files_df = [ str(output_directory_name.joinpath(f).resolve()) for f in list_files_dfnames ] # creating the complete path for each file\n",
    "list_df_real_positions = [pd.read_csv(f) for f in  path_files_df] # List with all the videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the simulated images\n",
    "rsp.VisualizerImage(list_videos,list_files_names=list_files_names,selected_channel =0,selected_timepoint= 0,normalize=0,individual_figure_size=7).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_DataFrame_particles_intensities= []\n",
    "list_array_intensities = []\n",
    "list_time_vector = []\n",
    "for i in tqdm(range(0,nimg)): \n",
    "    DataFrame_particles_intensities, array_intensities, time_vector, mean_intensities,std_intensities, mean_intensities_normalized, std_intensities_normalized = rsp.PipelineTracking(list_videos[i],particle_size=particle_detection_size,file_name=list_files_names[i],selected_channel=0,intensity_calculation_method =intensity_calculation_method, mask_selection_method = mask_selection_method,show_plot=1,use_optimization_for_tracking=use_optimization_for_tracking,real_positions_dataframe = list_df_real_positions[i],average_cell_diameter=200,print_process_times=1,min_percentage_time_tracking=min_percentage_time_tracking).run()    \n",
    "    list_DataFrame_particles_intensities.append(DataFrame_particles_intensities)\n",
    "    list_array_intensities.append(array_intensities)\n",
    "    list_time_vector.append(time_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing intensity distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Real\" intensities from SSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_timepoint = 0 #simulation_time_in_sec-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extrema(vector,min_percentile = 1 ,max_percentile = 99):\n",
    "    '''This function is intended to remove extrema data given by the min and max percentiles specified by the user'''\n",
    "    vector = vector [vector>0]\n",
    "    max_val = np.percentile(vector, max_percentile)\n",
    "    min_val =  np.percentile(vector, min_percentile)\n",
    "    print(round(min_val,2),round(max_val,2))\n",
    "    new_vector = vector [vector< max_val] # = np.percentile(vector,max_percentile)\n",
    "    new_vector = new_vector [new_vector> min_val] # = np.percentile(vector, min_percentile)\n",
    "    return new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssa_trajectories = np.load(str(trajectories_dir))\n",
    "ssa_trajectories_timePoint = ssa_trajectories[:,sel_timepoint].flatten()\n",
    "ssa_trajectories_timePoint= remove_extrema(ssa_trajectories_timePoint)\n",
    "ssa_trajectories_timePoint_normalized = (ssa_trajectories_timePoint-np.amin(ssa_trajectories_timePoint))/ (np.amax(ssa_trajectories_timePoint)-np.amin(ssa_trajectories_timePoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovered intensities from tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_green_int = np.array([])\n",
    "for i in range(0,nimg): \n",
    "    all_cells_green_int = np.append(all_cells_green_int,list_array_intensities[i][:,sel_timepoint,1].flatten())   \n",
    "all_cells_green_int = all_cells_green_int[all_cells_green_int>0]\n",
    "all_cells_green_int= remove_extrema(all_cells_green_int)\n",
    "all_cells_green_int_normalized = (all_cells_green_int-np.amin(all_cells_green_int))/ (np.amax(all_cells_green_int)-np.amin(all_cells_green_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading intensities from image. \"Perfect tracking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the number of real simulations from folder name\n",
    "path_str = str(output_directory_name)                    # path\n",
    "ind_str_start = path_str.find('_ns_') +4                # start index in string\n",
    "ind_str_end = path_str.find('_diff')                    # end index in string\n",
    "max_nspots = int(path_str[ind_str_start:ind_str_end])   # number of spots per cell\n",
    "\n",
    "#Pre-alocating memory\n",
    "intensity_values_in_image = np.zeros((nimg,max_nspots)) # prealocating memory for intentiy\n",
    "\n",
    "for i in range(0,nimg):\n",
    "    for j in range (0,max_nspots):\n",
    "        file_name = str(output_directory_name.joinpath('sim_cell_'+str(i)+'_df.csv'))\n",
    "        df_intensities_real = pd.read_csv(file_name)  \n",
    "        intensity_values_in_image[i,j] = df_intensities_real[df_intensities_real['particle'] ==j].green_int_mean.values[sel_timepoint] \n",
    "intensity_values_in_image_flat = intensity_values_in_image.flatten()\n",
    "intensity_values_in_image_flat =  intensity_values_in_image_flat[intensity_values_in_image_flat>0]\n",
    "intensity_values_in_image_flat= remove_extrema(intensity_values_in_image_flat)\n",
    "intensity_values_in_image_normalized = (intensity_values_in_image_flat-np.amin(intensity_values_in_image_flat))/ (np.amax(intensity_values_in_image_flat)-np.amin(intensity_values_in_image_flat)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity histograms with au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "axes[0].hist(ssa_trajectories_timePoint,bins=60,density=True, stacked=True, color='orangered' )     \n",
    "axes[0].set(title='Simulation')\n",
    "axes[0].set(xlabel='intensities (ump)')\n",
    "axes[0].set(ylabel='count')\n",
    "\n",
    "axes[1].hist(all_cells_green_int,bins=60,density=True, stacked=True, color='chartreuse' )     \n",
    "axes[1].set(title='Tracking')\n",
    "axes[1].set(xlabel='intensities (au)')\n",
    "axes[1].set(ylabel='count')\n",
    "\n",
    "axes[2].hist(intensity_values_in_image_flat,bins=60,density=True, stacked=True, color='cyan' )     \n",
    "axes[2].set(title='Image')\n",
    "axes[2].set(xlabel='intensities (au)')\n",
    "axes[2].set(ylabel='count')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing intensities to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X_{norm} = \\frac{X -min(X)}{max(X) - min(X)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting normalized intensities\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "axes[0].hist(ssa_trajectories_timePoint_normalized,bins=60,density=True, stacked=True, color='orangered' )     \n",
    "axes[0].set(title='Simulation')\n",
    "axes[0].set(xlabel='intensities (norm)')\n",
    "axes[0].set(ylabel='count')\n",
    "\n",
    "axes[1].hist(all_cells_green_int_normalized,bins=60,density=True, stacked=True, color='chartreuse' )     \n",
    "axes[1].set(title='Tracking')\n",
    "axes[1].set(xlabel='intensities (norm)')\n",
    "axes[1].set(ylabel='count')\n",
    "\n",
    "axes[2].hist(intensity_values_in_image_normalized,bins=60,density=True, stacked=True, color='cyan' )     \n",
    "axes[2].set(title='Image')\n",
    "axes[2].set(xlabel='intensities (au)')\n",
    "axes[2].set(ylabel='count')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cummulative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data1 = ssa_trajectories_timePoint_normalized\n",
    "data_sorted_1 = np.sort(data1)\n",
    "p_1 =np.linspace(0, 1, len(data1), endpoint=False)\n",
    "\n",
    "data2 = all_cells_green_int_normalized\n",
    "data_sorted_2 = np.sort(data2)\n",
    "p_2 =np.linspace(0, 1, len(data2), endpoint=False)\n",
    "\n",
    "data3 = intensity_values_in_image_normalized\n",
    "data_sorted_3 = np.sort(data3)\n",
    "p_3 =np.linspace(0, 1, len(data3), endpoint=False)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(data_sorted_1, p_1, 'orangered',linewidth=3,label ='Simulation')\n",
    "plt.plot(data_sorted_2, p_2,'chartreuse',linewidth=3,label ='tracking')\n",
    "plt.plot(data_sorted_3, p_3,'cyan',linewidth=3,label ='Image')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('cumfreq')\n",
    "plt.ylabel('Cumulative probability')\n",
    "plt.xlabel('Normalized intensity')\n",
    "plt.show()\n",
    "\n",
    "# Print number of spots\n",
    "print('Number of spots for Simulation:',len(data1))\n",
    "print('Number of spots recovered from tracking:',len(data2))\n",
    "print('Number of spots recovered from image:',len(data3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison using the KS-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Kolmogorov distance\n",
    "\n",
    "ks_distance = scipy.stats.kstest(data1,data2).statistic\n",
    "print('The KS-distance between SSA and tracking is:' , round(ks_distance,2))\n",
    "\n",
    "ks_distance = scipy.stats.kstest(data1,data3).statistic\n",
    "print('The KS-distance between SSA and image is:' , round(ks_distance,2))\n",
    "\n",
    "#ks_distance = scipy.stats.kstest(data3,data2).statistic\n",
    "#print('The KS-distance between image and tracking is:' , round(ks_distance,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison using the Anderson-Darling distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_distance,_,_ = scipy.stats.anderson_ksamp([data1,data2],midrank=False)\n",
    "print('The AD-distance between SSA and tracking is:' , round(ad_distance,2))\n",
    "\n",
    "ad_distance,_,_ = scipy.stats.anderson_ksamp([data1,data3],midrank=False)\n",
    "print('The AD-distance between SSA and image is:' , round(ad_distance,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "## Comparison using likelihood function\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LL_fun(real_data,simulation_data,nbins=30):\n",
    "    hist_exp_data, hist_exp_bins = np.histogram( real_data , bins=nbins)\n",
    "    dist_sim_data, dist_sim_bins = np.histogram(simulation_data, bins=hist_exp_bins, density=True)\n",
    "    dist_sim_data[dist_sim_data ==0] = 1e-7\n",
    "    LL_int_distb = np.dot(hist_exp_data,np.log(dist_sim_data))    # likelihood function for comparing distributions\n",
    "    return LL_int_distb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOG LIKELIHOOD OF THE INTENSITY DISTRIBUTIONS\n",
    "\n",
    "LL_ssa_tracking = LL_fun(real_data= data_sorted_1,simulation_data=data_sorted_2,nbins=40)\n",
    "print('The Likelihood between SSA and tracking is:' , round(LL_ssa_tracking,2))\n",
    "\n",
    "LL_ssa_img = LL_fun(real_data=data_sorted_1, simulation_data=data_sorted_3,nbins=40)\n",
    "print('The Likelihood between SSA and image is:' , round(LL_ssa_img,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "#axs[0].set_title(\"Red\")\n",
    "#axs[0].set_xlabel(\"SNR\")\n",
    "#axs[0].set_ylabel(\"Frequency\")\n",
    "#axs[0].hist(df_intensities_real.SNR_red.values,bins=30)\n",
    "\n",
    "#axs[1].set_title(\"Green\")\n",
    "#axs[1].set_xlabel(\"SNR\")\n",
    "#axs[1].set_ylabel(\"Frequency\")\n",
    "#axs[1].hist(df_intensities_real.SNR_green.values,bins=30)\n",
    "#try:\n",
    "#    axs[2].set_title(\"Blue\")\n",
    "#    axs[2].set_xlabel(\"SNR\")\n",
    "#    axs[2].set_ylabel(\"Frequency\")\n",
    "#    axs[2].hist(df_intensities_real.SNR_blue.values,bins=30)\n",
    "#except:\n",
    "#    pass\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff7d081ce4817123eaa831f172a3e70559f334d78f7532d1a0ec42cef63090fa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
